{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Nah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf  # I am using tensorflow=2.1\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np\n",
    "import os\n",
    "from src.utils import *\n",
    "from src.score import *\n",
    "from src.data_generator import *\n",
    "from src.networks import *\n",
    "from src.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PeriodicPadding2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, pad_width, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_width = pad_width\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.pad_width == 0:\n",
    "            return inputs\n",
    "        inputs_padded = tf.concat(\n",
    "            [inputs[:, :, -self.pad_width:, :], inputs, inputs[:, :, :self.pad_width, :]], axis=2)\n",
    "        # Zero padding in the lat direction\n",
    "        inputs_padded = tf.pad(inputs_padded, [[0, 0], [self.pad_width, self.pad_width], [0, 0], [0, 0]])\n",
    "        return inputs_padded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'pad_width': self.pad_width})\n",
    "        return config\n",
    "\n",
    "class PeriodicConv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters,\n",
    "                 kernel_size,\n",
    "                 conv_kwargs={},\n",
    "                 **kwargs, ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_kwargs = conv_kwargs\n",
    "        self.use_bias = False\n",
    "        self.kernel_regularizer = None\n",
    "        self.stride = 1\n",
    "        if 'use_bias' in conv_kwargs:\n",
    "            self.use_bias = conv_kwargs['use_bias']\n",
    "        if 'kernel_regularizer' in conv_kwargs:\n",
    "            self.kernel_regularizer = conv_kwargs['kernel_regularizer']\n",
    "        if 'stride' in conv_kwargs:\n",
    "            self.stride = conv_kwargs['stride']\n",
    "\n",
    "        if type(kernel_size) is not int:\n",
    "            assert kernel_size[0] == kernel_size[1], 'PeriodicConv2D only works for square kernels'\n",
    "            kernel_size = kernel_size[0]\n",
    "        pad_width = (kernel_size - 1) // 2\n",
    "        self.padding = PeriodicPadding2D(pad_width)\n",
    "        #self.conv = Conv2D(\n",
    "        #    #filters, kernel_size, padding='valid', **conv_kwargs\n",
    "        #)\n",
    "        #self.transpose_conv=Conv2DTranspose(\n",
    "        #    #filters, kernel_size, padding='valid', **conv_kwargs\n",
    "        #)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        paddinginputs = self.padding(inputs)\n",
    "        inputshape = paddinginputs.get_shape()\n",
    "        weight = tf.compat.v1.get_variable(name = 'weights', shape=[self.kernel_size,self.kernel_size,inputshape[-1],self.filters], initializer = tf.keras.initializers.glorot_normal, regularizer=self.kernel_regularizer)\n",
    "        conv = tf.nn.conv2d(paddinginputs, weight, strides=self.stride,padding='VALID')\n",
    "        paddinginput_back = conv\n",
    "        if self.use_bias:\n",
    "            bias = tf.compat.v1.get_variable(name = 'bias', shape=[self.filters], initializer = tf.keras.initializers.glorot_normal)\n",
    "            conv += bias\n",
    "        paddinginput_back = tf.nn.conv2d_transpose(paddinginput_back, weight,tf.shape(paddinginputs), self.stride, padding = 'VALID')\n",
    "        rrloss = tf.reduce_mean(tf.nn.l2_loss(paddinginputs - paddinginput_back))\n",
    "        return conv, rrloss\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'filters': self.filters, 'kernel_size': self.kernel_size, 'conv_kwargs': self.conv_kwargs})\n",
    "        return config\n",
    "\n",
    "def convblock(inputs, filters, kernel=3, stride=1, bn_position=None, l2=0,\n",
    "              use_bias=True, dropout=0, activation='relu'):\n",
    "    x = inputs\n",
    "    if bn_position == 'pre': x = BatchNormalization()(x)\n",
    "    x, rrloss_p = PeriodicConv2D(\n",
    "        filters,  kernel, conv_kwargs={\n",
    "            'kernel_regularizer': regularizers.l2(l2),\n",
    "            'use_bias': use_bias,\n",
    "        }\n",
    "    )(x)\n",
    "    if bn_position == 'mid': x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x) if activation == 'leakyrelu' else Activation(activation)(x)\n",
    "    if bn_position == 'post': x = BatchNormalization()(x)\n",
    "    if dropout > 0: x = Dropout(dropout)(x)\n",
    "    return x, rrloss_p\n",
    "\n",
    "def resblock(inputs, filters, kernel, bn_position=None, l2=0, use_bias=True,\n",
    "             dropout=0, skip=True, activation='relu', down=False, up=False):\n",
    "    rrloss = tf.constant(0.)\n",
    "    x = inputs\n",
    "    if down:\n",
    "        x = MaxPooling2D()(x)\n",
    "    for i in range(2):\n",
    "        with tf.compat.v1.variable_scope(\"reslayer_%d\"%i):\n",
    "            x, rrloss_p = convblock(\n",
    "                x, filters, kernel, bn_position=bn_position, l2=l2, use_bias=use_bias,\n",
    "                dropout=dropout, activation=activation\n",
    "            )\n",
    "            rrloss += rrloss_p\n",
    "    if down or up:\n",
    "        with tf.compat.v1.variable_scope(\"reslayer_%s\"%(\"down\" if down else \"up\")):\n",
    "            inputs, rrloss_p = PeriodicConv2D(\n",
    "                filters,  kernel, conv_kwargs={\n",
    "                    'kernel_regularizer': regularizers.l2(l2),\n",
    "                    'use_bias': use_bias,\n",
    "                    'strides': 2 if down else 1\n",
    "                }\n",
    "            )(inputs)\n",
    "            rrloss += rrloss_p\n",
    "    if skip: x = Add()([inputs, x])\n",
    "    return x, rrloss\n",
    "\n",
    "#def build_resnet(input, filters, kernels, input_shape, bn_position=None, use_bias=True, l2=0,\n",
    "def build_resnet(input, filters, kernels, bn_position=None, use_bias=True, l2=0,\n",
    "                 skip=True, dropout=0, activation='relu', long_skip=False,\n",
    "                 **kwargs):\n",
    "    x = input# = Input(shape=input_shape)\n",
    "    rrloss = tf.constant(0.)\n",
    "\n",
    "    # First conv block to get up to shape\n",
    "    with tf.compat.v1.variable_scope(\"head\"):\n",
    "        x, rrloss_p = convblock(\n",
    "            x, filters[0], kernels[0], bn_position=bn_position, l2=l2, use_bias=use_bias,\n",
    "            dropout=dropout, activation=activation\n",
    "        )\n",
    "        rrloss += rrloss_p\n",
    "        ls = x\n",
    "\n",
    "    # Resblocks\n",
    "    layerindex = 0\n",
    "    for f, k in zip(filters[1:-1], kernels[1:-1]):\n",
    "        with tf.compat.v1.variable_scope(\"resblock_%d\"%layerindex):\n",
    "            x, rrloss_p = resblock(x, f, k, bn_position=bn_position, l2=l2, use_bias=use_bias,\n",
    "                    dropout=dropout, skip=skip, activation=activation)\n",
    "            rrloss += rrloss_p\n",
    "            if long_skip:\n",
    "                x = Add()([x, ls])\n",
    "        layerindex += 1\n",
    "\n",
    "    with tf.compat.v1.variable_scope(\"end\"):\n",
    "        # Final convolution\n",
    "        output, rrloss_p = PeriodicConv2D(\n",
    "            filters[-1], kernels[-1],\n",
    "            conv_kwargs={'kernel_regularizer': regularizers.l2(l2)},\n",
    "        )(x)\n",
    "        rrloss += rrloss_p\n",
    "\n",
    "    # This is just because I am using mixed precision. Can be left out for regular precision.\n",
    "    output = Activation('linear', dtype='float32')(output)\n",
    "    #return keras.models.Model(input, output), rrloss\n",
    "    return output, rrloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "args = load_args('../nn_configs/C/017-resnet_d3_ztt_3d.yml')\n",
    "args['train_years'] = ['2015', '2015']\n",
    "args['valid_years'] = ['2015', '2015']\n",
    "args['test_years'] = ['2015', '2015']\n",
    "args['train_tfr_files'] = None\n",
    "args['test_tfr_files'] = None\n",
    "args['tvalid_tfr_files'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dg_test = load_data(**args, only_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=dg_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output, rrloss = build_resnet(\n",
    "    inputs,\n",
    "    filters=[128, 128, 128, 3],\n",
    "    kernels=[7, 3, 3, 3],\n",
    "    #input_shape=(32, 64, 114,),\n",
    "    bn_position='post',\n",
    "    dropout=0.1,   # I am currently using a combination of dropout and l2 for regularization\n",
    "    l2=1e-5,       # Of course it would be great if I didn't have to use them with racecar\n",
    "    activation='leakyrelu',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre for You"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import pdb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, ds, var_dict, lead_time, batch_size=32, shuffle=True, load=True,\n",
    "                 mean=None, std=None, output_vars=None, data_subsample=1, norm_subsample=1,\n",
    "                 nt_in=1, dt_in=1, cont_time=False, fixed_time=False, multi_dt=1, verbose=0,\n",
    "                 min_lead_time=None, las_kernel=None, las_gauss_std=None, normalize=True,\n",
    "                 tfrecord_files=None, tfr_buffer_size=1000, tfr_num_parallel_calls=1,\n",
    "                 cont_dt=1, tfr_prefetch=None, tfr_repeat=True, y_roll=None, X_roll=None,\n",
    "                 discard_first=None, tp_log=None, tfr_out=False, tfr_out_idxs=None,\n",
    "                 old_const=False, is_categorical=False, num_bins=50, bin_min=-5, bin_max=5,\n",
    "                 predict_difference=False, adaptive_bins=None):\n",
    "        \"\"\"\n",
    "        Data generator for WeatherBench data.\n",
    "        Template from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "        Args:\n",
    "            ds: Dataset containing all variables\n",
    "            var_dict: Dictionary of the form {'var': level}. Use None for level if data is of single level\n",
    "            lead_time: Lead time in hours\n",
    "            batch_size: Batch size\n",
    "            shuffle: bool. If True, data is shuffled.\n",
    "            load: bool. If True, datadet is loaded into RAM.\n",
    "            mean: If None, compute mean from data.\n",
    "            std: If None, compute standard deviation from data.\n",
    "            data_subsample: Only take every ith time step\n",
    "            norm_subsample: Same for normalization. This is AFTER data_subsample!\n",
    "            nt_in: How many time steps for input. AFTER data_subsample!\n",
    "            dt_in: Interval of input time steps. AFTER data_subsample!\n",
    "        \"\"\"\n",
    "        if verbose: print('DG start', datetime.datetime.now().time())\n",
    "        self.ds = ds\n",
    "        self.var_dict = var_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lead_time = lead_time\n",
    "        self.nt_in = nt_in\n",
    "        self.dt_in = dt_in\n",
    "        self.cont_time = cont_time\n",
    "        self.min_lead_time = min_lead_time\n",
    "        self.fixed_time = fixed_time\n",
    "        self.multi_dt = multi_dt\n",
    "        self.tfrecord_files = tfrecord_files\n",
    "        self.normalize = normalize\n",
    "        self.tfr_num_parallel_calls = tfr_num_parallel_calls\n",
    "        self.tfr_buffer_size = tfr_buffer_size\n",
    "        self.cont_dt = cont_dt\n",
    "        self.tfr_prefetch = tfr_prefetch\n",
    "        self.tfr_repeat = tfr_repeat\n",
    "        self.tfr_out = tfr_out\n",
    "        self.y_roll = y_roll\n",
    "        self.X_roll = X_roll\n",
    "        self.tfr_max_lead = 120\n",
    "        self.tfr_out_idxs = tfr_out_idxs\n",
    "        self.old_const = old_const\n",
    "        self.is_categorical = is_categorical\n",
    "        self.num_bins = num_bins\n",
    "        self.bin_min = bin_min\n",
    "        self.bin_max = bin_max\n",
    "        self.predict_difference = predict_difference\n",
    "        if self.predict_difference:\n",
    "            assert self.tfrecord_files is None, 'difference does not work for tfr'\n",
    "        self.adaptive_bins = adaptive_bins\n",
    "\n",
    "        data = []\n",
    "        level_names = []\n",
    "        generic_level = xr.DataArray([1], coords={'level': [1]}, dims=['level'])\n",
    "        for long_var, params in var_dict.items():\n",
    "            if long_var == 'constants':\n",
    "                for var in params:\n",
    "                    data.append(ds[var].expand_dims(\n",
    "                        {'level': generic_level, 'time': ds.time}, (1, 0)\n",
    "                    ))\n",
    "                    level_names.append(var)\n",
    "            else:\n",
    "                var, levels = params\n",
    "                da = ds[var]\n",
    "                if tp_log and var == 'tp':\n",
    "                    da = log_trans(da, tp_log)\n",
    "                try:\n",
    "                    data.append(da.sel(level=levels))\n",
    "                    level_names += [f'{var}_{level}' for level in levels]\n",
    "                except ValueError:\n",
    "                    data.append(da.expand_dims({'level': generic_level}, 1))\n",
    "                    level_names.append(var)\n",
    "\n",
    "        self.data = xr.concat(data, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "        if discard_first is not None:\n",
    "            self.data = self.data.isel(time=slice(discard_first, None))\n",
    "        self.data['level_names'] = xr.DataArray(\n",
    "            level_names, dims=['level'], coords={'level': self.data.level})\n",
    "        if output_vars is None:\n",
    "            self.output_idxs = range(len(self.data.level))\n",
    "        else:\n",
    "            self.output_idxs = [i for i, l in enumerate(self.data.level_names.values)\n",
    "                                if any([bool(re.match(o, l)) for o in output_vars])]\n",
    "        self.const_idxs = [i for i, l in enumerate(self.data.level_names) if l in var_dict['constants']]\n",
    "        self.not_const_idxs = [i for i, l in enumerate(self.data.level_names) if l not in var_dict['constants']]\n",
    "\n",
    "        # Subsample\n",
    "        self.data = self.data.isel(time=slice(0, None, data_subsample))\n",
    "        self.raw_data = self.data\n",
    "        self.dt = self.data.time.diff('time')[0].values / np.timedelta64(1, 'h')\n",
    "        self.dt_in = int(self.dt_in // self.dt)\n",
    "        self.nt_offset = (nt_in - 1) * self.dt_in\n",
    "\n",
    "        if self.min_lead_time is None:\n",
    "            self.min_nt = 1\n",
    "        else:\n",
    "            self.min_nt = int(self.min_lead_time / self.dt)\n",
    "\n",
    "        # Normalize\n",
    "        if verbose: print('DG normalize', datetime.datetime.now().time())\n",
    "        if mean is not None:\n",
    "            self.mean = mean\n",
    "        else:\n",
    "            self.mean = self.data.isel(time=slice(0, None, norm_subsample)).mean(\n",
    "                ('time', 'lat', 'lon')).compute()\n",
    "            if 'tp' in self.data.level_names:  # set tp mean to zero but not if ext\n",
    "                tp_idx = list(self.data.level_names).index('tp')\n",
    "                self.mean.values[tp_idx] = 0\n",
    "\n",
    "        if std is not None:\n",
    "            self.std = std\n",
    "        else:\n",
    "            self.std = self.data.isel(time=slice(0, None, norm_subsample)).std(\n",
    "                ('time', 'lat', 'lon')).compute()\n",
    "        if tp_log is not None:\n",
    "            self.mean.attrs['tp_log'] = tp_log\n",
    "            self.std.attrs['tp_log'] = tp_log\n",
    "        if normalize:\n",
    "            self.data = (self.data - self.mean) / self.std\n",
    "\n",
    "        if verbose: print('DG load', datetime.datetime.now().time())\n",
    "        if load:\n",
    "            if verbose: print('Loading data into RAM')\n",
    "            self.data.load()\n",
    "        if verbose: print('DG done', datetime.datetime.now().time())\n",
    "\n",
    "        if self.X_roll is not None:\n",
    "            self.X_roll = int(self.X_roll // self.dt)\n",
    "            self.X_rolled = self.data.rolling(time=self.X_roll).mean()\n",
    "            self.nt_offset += self.X_roll\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        if self.y_roll is not None:\n",
    "            self.y_roll = int(self.y_roll // self.dt)\n",
    "            assert self.y_roll < self.nt, 'nt must be larger than y_roll'\n",
    "            self.y_rolled = self.data.isel(level=self.output_idxs).rolling(time=self.y_roll).mean()\n",
    "\n",
    "        if self.tfrecord_files is not None:\n",
    "            self.is_tfr = True\n",
    "            self._setup_tfrecord_ds()\n",
    "        else:\n",
    "            self.is_tfr = False\n",
    "            self.tfr_dataset = None\n",
    "\n",
    "        if self.is_categorical:\n",
    "            self.bins = np.linspace(self.bin_min, self.bin_max, self.num_bins+1)\n",
    "            self.bins[0] = -np.inf; self.bins[-1] = np.inf  # for rare out-of-bound cases.\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.idxs = np.arange(self.nt_offset, self.n_samples)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.idxs)\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(len(self.idxs) / self.batch_size))\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return (\n",
    "            len(self.data.lat),\n",
    "            len(self.data.lon),\n",
    "            len(self.data.level.isel(level=self.not_const_idxs)) * self.nt_in + len(self.data.level.isel(\n",
    "                level=self.const_idxs)) + self.cont_time\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def nt(self):\n",
    "        assert (self.lead_time / self.dt).is_integer(), \"lead_time and dt not compatible.\"\n",
    "        return int(self.lead_time / self.dt)\n",
    "\n",
    "    @property\n",
    "    def init_time(self):\n",
    "        stop = -self.nt\n",
    "        if self.is_tfr:\n",
    "            stop += -(self.tfr_max_lead - self.lead_time // self.dt)\n",
    "        return self.data.isel(time=slice(self.nt_offset, int(stop))).time\n",
    "\n",
    "    @property\n",
    "    def valid_time(self):\n",
    "        start = self.nt+self.nt_offset\n",
    "        stop = None\n",
    "        if self.multi_dt > 1:\n",
    "            diff = self.nt - self.nt // self.multi_dt\n",
    "            start -= diff; stop = -diff\n",
    "        if self.is_tfr:\n",
    "            stop = -int((self.tfr_max_lead - self.lead_time) // self.dt)\n",
    "            if stop == 0:\n",
    "                stop = None\n",
    "        return self.data.isel(time=slice(start, stop)).time\n",
    "\n",
    "    @property\n",
    "    def n_samples(self):\n",
    "        return self.data.isel(time=slice(0, -self.nt)).shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.tfrecord_files is None:\n",
    "            if hasattr(self, 'cheat'):\n",
    "                X, y = self._get_item(i)\n",
    "                return X, y[-1]\n",
    "            else:\n",
    "                return self._get_item(i)\n",
    "        else:\n",
    "            return self._get_tfrecord_item(i)\n",
    "\n",
    "    def _get_item(self, i):\n",
    "        'Generate one batch of data'\n",
    "        idxs = self.idxs[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "\n",
    "        if self.cont_time:\n",
    "            if not self.fixed_time:\n",
    "                nt = np.random.randint(self.min_nt, self.nt + 1, len(idxs))\n",
    "            else:\n",
    "                nt = np.ones(len(idxs), dtype='int') * self.nt\n",
    "            ftime = (nt * self.dt / 100)[:, None, None] * np.ones((1, len(self.data.lat),\n",
    "                                                                   len(self.data.lon)))\n",
    "        else:\n",
    "            nt = self.nt\n",
    "\n",
    "        if self.X_roll is not None:\n",
    "            X_data = self.X_rolled\n",
    "        else:\n",
    "            X_data = self.data\n",
    "\n",
    "        X = X_data.isel(time=idxs).values.astype('float32')\n",
    "\n",
    "        if self.multi_dt > 1: consts = X[..., self.const_idxs]\n",
    "\n",
    "        if self.nt_in > 1:\n",
    "            if self.old_const:\n",
    "                X = np.concatenate([\n",
    "                                       self.data.isel(time=idxs - nt_in * self.dt_in).values\n",
    "                                       for nt_in in range(self.nt_in - 1, 0, -1)\n",
    "                                   ] + [X], axis=-1).astype('float32')\n",
    "            else:\n",
    "                X = np.concatenate([\n",
    "                                       X_data.isel(time=idxs - nt_in * self.dt_in).values[..., self.not_const_idxs]\n",
    "                                       for nt_in in range(self.nt_in - 1, 0, -1)\n",
    "                                   ] + [X], axis=-1).astype('float32')\n",
    "\n",
    "        if self.multi_dt > 1:\n",
    "            X = [X[..., self.not_const_idxs], consts]\n",
    "            step = self.nt // self.multi_dt\n",
    "            y = [\n",
    "                self.data.isel(time=idxs + nt, level=self.output_idxs).values.astype('float32')\n",
    "                for nt in np.arange(step, self.nt + step, step)\n",
    "            ]\n",
    "        elif self.y_roll is not None:\n",
    "            y = self.y_rolled.isel(\n",
    "                time=idxs + nt,\n",
    "            ).values.astype('float32')\n",
    "        elif self.tfr_out:\n",
    "            assert self.batch_size == 1, 'bs must be one'\n",
    "            time_slice = slice(idxs[0]+self.min_nt, idxs[0]+self.nt+1)\n",
    "            y = self.data.isel(time=time_slice, level=self.output_idxs).values.astype('float32')[None]\n",
    "        elif self.predict_difference:\n",
    "            y = (\n",
    "                self.data.isel(time=idxs + nt, level=self.output_idxs).values -\n",
    "                self.data.isel(time=idxs, level=self.output_idxs).values\n",
    "            ).astype('float32')\n",
    "        else:\n",
    "            y = self.data.isel(time=idxs + nt, level=self.output_idxs).values.astype('float32')\n",
    "\n",
    "        if self.is_categorical:\n",
    "            y_shape = y.shape\n",
    "            y = pd.cut(y.reshape(-1), self.bins, labels=False).reshape(y_shape)\n",
    "            y = tf.keras.utils.to_categorical(y, num_classes=self.num_bins)\n",
    "\n",
    "        if self.cont_time:\n",
    "            X = np.concatenate([X, ftime[..., None]], -1).astype('float32')\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def _decode(self, example_proto):\n",
    "        dic = _parse(example_proto)\n",
    "        X = tf.io.parse_tensor(dic['X'], np.float32)\n",
    "        y = tf.io.parse_tensor(dic['y'], np.float32)\n",
    "        if self.tfr_out_idxs is not None:\n",
    "            y = tf.gather(y, self.tfr_out_idxs, axis=-1)\n",
    "        if self.cont_time:\n",
    "            if self.fixed_time:\n",
    "                y_idx = self.nt-1\n",
    "            else:\n",
    "                y_idx = tf.random.uniform((), self.min_nt-1, self.nt, dtype=tf.int32)\n",
    "            y_time = (y_idx+1) * self.dt\n",
    "            ftime = (y_time / 100) * np.ones((len(self.data.lat), len(self.data.lon), 1))\n",
    "            X = tf.concat([X, tf.cast(ftime, tf.float32)], -1)\n",
    "            return X, y[y_idx]\n",
    "        else:\n",
    "            y_idx = self.nt-1\n",
    "            return X, y[y_idx]\n",
    "\n",
    "    def _setup_tfrecord_ds(self):\n",
    "        # Find all files to be used\n",
    "        if type(self.tfrecord_files) is list:\n",
    "            tfr_fns = self.tfrecord_files\n",
    "        else:\n",
    "            tfr_fns = sorted(glob(self.tfrecord_files))\n",
    "\n",
    "        dataset = tf.data.TFRecordDataset(\n",
    "            tfr_fns, num_parallel_reads=self.tfr_num_parallel_calls\n",
    "        ).map(self._decode)\n",
    "\n",
    "        if self.shuffle:\n",
    "            dataset = dataset.shuffle(\n",
    "                buffer_size=self.tfr_buffer_size, reshuffle_each_iteration=True\n",
    "            )\n",
    "\n",
    "        self.tfr_dataset = dataset.batch(self.batch_size)\n",
    "        # if self.tfr_repeat:\n",
    "        #     self.tfr_dataset = self.tfr_dataset.repeat()\n",
    "        if self.tfr_prefetch is not None:\n",
    "            self.tfr_dataset = self.tfr_dataset.prefetch(self.tfr_prefetch)\n",
    "        self.tfr_dataset_np = self.tfr_dataset.as_numpy_iterator()\n",
    "\n",
    "\n",
    "    def _get_tfrecord_item(self, i):\n",
    "        X, y = next(self.tfr_dataset_np)\n",
    "        return X, y\n",
    "\n",
    "    def to_tfr(self, savedir, steps_per_file=250):\n",
    "        assert self.batch_size == 1, 'bs must be one'\n",
    "        for i, (X, y) in tqdm(enumerate(self)):\n",
    "            if i % steps_per_file == 0:\n",
    "                c = int(np.floor(i / steps_per_file))\n",
    "                fn = f'{savedir}/{str(c).zfill(3)}.tfrecord'\n",
    "                print('Writing to file:', fn)\n",
    "                writer = tf.io.TFRecordWriter(fn)\n",
    "            serialized_example = serialize_example(X[0], y[0])  # Remove batch dimension\n",
    "            writer.write(serialized_example)\n",
    "            if i + 1 % steps_per_file == 0:\n",
    "                writer.close()\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/data/stephan/WeatherBench/5.625deg/'\n",
    "var_dict = {\n",
    "    'geopotential': ('z', [50, 250, 500, 600, 700, 850, 925]), \n",
    "    'temperature': ('t', [50, 250, 500, 600, 700, 850, 925]), \n",
    "    'u_component_of_wind': ('u', [50, 250, 500, 600, 700, 850, 925]), \n",
    "    'v_component_of_wind': ('v', [50, 250, 500, 600, 700, 850, 925]), \n",
    "    'specific_humidity': ('q', [50, 250, 500, 600, 700, 850, 925]), \n",
    "    'toa_incident_solar_radiation': ('tisr', None), \n",
    "    '2m_temperature': ('t2m', None), \n",
    "    '6hr_precipitation': ('tp', None), \n",
    "    'constants': ['lsm','orography','lat2d']\n",
    "}\n",
    "output_vars = ['z_500', 't_850', 't2m']\n",
    "lead_time = 72\n",
    "data_subsample = 2\n",
    "norm_subsample = 30000\n",
    "nt = 3\n",
    "dt = 6\n",
    "discard_first = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.merge(\n",
    "    [xr.open_mfdataset(f'{datadir}/{var}/*.nc', combine='by_coords')\n",
    "     for var in var_dict.keys()],\n",
    "    fill_value=0  # For the 'tisr' NaNs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_years = ['2015', '2015']  # For full training data, use ['1979', '2015']. Will use 200GB of RAM.\n",
    "valid_years = ['2016', '2016']\n",
    "test_years = ['2017', '2018']\n",
    "ds_train = ds.sel(time=slice(*train_years))\n",
    "ds_valid = ds.sel(time=slice(*valid_years))\n",
    "ds_test = ds.sel(time=slice(*test_years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_train = DataGenerator(\n",
    "    ds_train,\n",
    "    var_dict,\n",
    "    lead_time,\n",
    "    output_vars=output_vars,\n",
    "    data_subsample=data_subsample,\n",
    "    norm_subsample=norm_subsample,\n",
    "    nt_in=nt,\n",
    "    dt_in=dt,\n",
    "    discard_first=discard_first\n",
    ")\n",
    "# dg_valid = DataGenerator(\n",
    "#     ds_valid,\n",
    "#     var_dict,\n",
    "#     lead_time,\n",
    "#     output_vars=output_vars,\n",
    "#     data_subsample=data_subsample,\n",
    "#     norm_subsample=norm_subsample,\n",
    "#     nt_in=nt,\n",
    "#     dt_in=dt,\n",
    "#     discard_first=discard_first,\n",
    "#     mean=dg_train.mean,\n",
    "#     std=dg_train.std\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 64, 117) (32, 32, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "X, y = dg_train[0]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lat_mse(lat):\n",
    "    weights_lat = np.cos(np.deg2rad(lat)).values\n",
    "    weights_lat /= weights_lat.mean()\n",
    "    def lat_mse(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        mse = error**2 * weights_lat[None, : , None, None]\n",
    "        return mse\n",
    "    return lat_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_mse = create_lat_mse(dg_train.data.lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
